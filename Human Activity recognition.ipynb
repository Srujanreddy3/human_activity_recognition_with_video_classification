{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e298bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dancing', 'exercise', 'yoga']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\train')\n",
    "\n",
    "label_types = os.listdir(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\train')\n",
    "print (label_types)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "039bea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tag                                         video_name\n",
      "0  dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "1  dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "2  dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "3  dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "4  dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "     tag                                         video_name\n",
      "13  yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "14  yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "15  yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "16  yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "17  yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n"
     ]
    }
   ],
   "source": [
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " all_rooms = os.listdir(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\train' + '/' +item)\n",
    "\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\train' + '/' +item) + '/' + room))\n",
    "          \n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49f5c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff25225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dancing', 'exercise', 'yoga']\n",
      "Types of activities found:  3\n",
      "        tag                                         video_name\n",
      "0   dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "1   dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "2   dancing  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "3  exercise  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "4  exercise  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "        tag                                         video_name\n",
      "4  exercise  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "5  exercise  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "6      yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "7      yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n",
      "8      yoga  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.listdir(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\test')\n",
    "print(dataset_path)\n",
    "\n",
    "room_types = os.listdir(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\test')\n",
    "print(\"Types of activities found: \", len(dataset_path))\n",
    "\n",
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " all_rooms = os.listdir(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\test' + '/' +item)\n",
    "\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str(r'C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\test' + '/' +item) + '/' + room))\n",
    "    \n",
    "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(test_df.head())\n",
    "print(test_df.tail())\n",
    "\n",
    "df = test_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c43c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af36d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e341e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 18\n",
      "Total videos for testing: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...</td>\n",
       "      <td>yoga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                         video_name       tag\n",
       "10          10  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...  exercise\n",
       "12          12  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...      yoga\n",
       "11          11  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...  exercise\n",
       "15          15  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...      yoga\n",
       "3            3  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...   dancing\n",
       "9            9  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...  exercise\n",
       "0            0  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...   dancing\n",
       "16          16  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...      yoga\n",
       "5            5  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...   dancing\n",
       "17          17  C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\da...      yoga"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5f316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4520c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9701a9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dancing', 'exercise', 'yoga']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8245cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52c2cc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (18, 20, 2048)\n",
      "Frame masks in train set: (18, 20)\n",
      "train_labels in train set: (18, 1)\n",
      "test_labels in train set: (9, 1)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    \n",
    "    labels = df[\"tag\"].values\n",
    "    \n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"train_labels in train set: {train_labels.shape}\")\n",
    "\n",
    "print(f\"test_labels in train set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7577169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1519 - accuracy: 0.3333\n",
      "Epoch 1: val_loss improved from inf to 0.98336, saving model to ./tmp\\video_classifier\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.1519 - accuracy: 0.3333 - val_loss: 0.9834 - val_accuracy: 0.8333\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.4167\n",
      "Epoch 2: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0128 - accuracy: 0.4167 - val_loss: 1.0994 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9891 - accuracy: 0.5833\n",
      "Epoch 3: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9891 - accuracy: 0.5833 - val_loss: 1.1874 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7782 - accuracy: 0.5833\n",
      "Epoch 4: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7782 - accuracy: 0.5833 - val_loss: 1.2150 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8038 - accuracy: 0.6667\n",
      "Epoch 5: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8038 - accuracy: 0.6667 - val_loss: 1.2279 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8600 - accuracy: 0.5833\n",
      "Epoch 6: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.8600 - accuracy: 0.5833 - val_loss: 1.1985 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8023 - accuracy: 0.6667\n",
      "Epoch 7: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8023 - accuracy: 0.6667 - val_loss: 1.1697 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7513 - accuracy: 0.9167\n",
      "Epoch 8: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.7513 - accuracy: 0.9167 - val_loss: 1.1632 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7479 - accuracy: 0.8333\n",
      "Epoch 9: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.7479 - accuracy: 0.8333 - val_loss: 1.1607 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6589 - accuracy: 0.7500\n",
      "Epoch 10: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.6589 - accuracy: 0.7500 - val_loss: 1.1655 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7757 - accuracy: 0.8333\n",
      "Epoch 11: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.7757 - accuracy: 0.8333 - val_loss: 1.1799 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.8333\n",
      "Epoch 12: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6885 - accuracy: 0.8333 - val_loss: 1.1940 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.8333\n",
      "Epoch 13: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.6428 - accuracy: 0.8333 - val_loss: 1.2126 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.7500\n",
      "Epoch 14: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6676 - accuracy: 0.7500 - val_loss: 1.2297 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.6667\n",
      "Epoch 15: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6949 - accuracy: 0.6667 - val_loss: 1.2488 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.6667\n",
      "Epoch 16: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6972 - accuracy: 0.6667 - val_loss: 1.2703 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6819 - accuracy: 0.6667\n",
      "Epoch 17: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.6819 - accuracy: 0.6667 - val_loss: 1.2939 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.8333\n",
      "Epoch 18: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.6643 - accuracy: 0.8333 - val_loss: 1.3166 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6754 - accuracy: 0.7500\n",
      "Epoch 19: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.6754 - accuracy: 0.7500 - val_loss: 1.3321 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.9167\n",
      "Epoch 20: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7014 - accuracy: 0.9167 - val_loss: 1.3427 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 1.0000\n",
      "Epoch 21: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5687 - accuracy: 1.0000 - val_loss: 1.3557 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6127 - accuracy: 0.9167\n",
      "Epoch 22: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6127 - accuracy: 0.9167 - val_loss: 1.3691 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.8333\n",
      "Epoch 23: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5750 - accuracy: 0.8333 - val_loss: 1.3832 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 1.0000\n",
      "Epoch 24: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5587 - accuracy: 1.0000 - val_loss: 1.3917 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.9167\n",
      "Epoch 25: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.6657 - accuracy: 0.9167 - val_loss: 1.3987 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.8333\n",
      "Epoch 26: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6334 - accuracy: 0.8333 - val_loss: 1.4063 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 1.0000\n",
      "Epoch 27: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5159 - accuracy: 1.0000 - val_loss: 1.4189 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.7500\n",
      "Epoch 28: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5442 - accuracy: 0.7500 - val_loss: 1.4284 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6757 - accuracy: 0.9167\n",
      "Epoch 29: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.6757 - accuracy: 0.9167 - val_loss: 1.4339 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.8333\n",
      "Epoch 30: val_loss did not improve from 0.98336\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5437 - accuracy: 0.8333 - val_loss: 1.4415 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1135 - accuracy: 0.2222\n",
      "Test accuracy: 22.22%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cee7f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: C:\\Users\\Hp\\OneDrive\\Desktop\\human activity\\data set\\test/dancing/dance-3.mp4\n",
      "  yoga: 37.33%\n",
      "  dancing: 35.31%\n",
      "  exercise: 27.36%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4b687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce340f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c03656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f412d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070bbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556ca4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
